{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython3", 
            "name": "python", 
            "version": "3.5.2"
        }, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "language": "python", 
            "name": "python3-spark21"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Team 13 - Big Data Project"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Dataset used: http://jmcauley.ucsd.edu/data/amazon/\nFormat: csv\nInfo: This dataset has book ratings from the Amazon Marketplace. The columns included are user, item, rating, timestamp (in epoch)."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "We uploaded the books dataset into the Jupyter Notebook. The dataset did not include the column headers, so we renamed the columns to the appropriate headers. We have shown with the first 5 rows that the dataset has been imported correctly."
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 4, 
            "source": "import ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'api_key': 'mI-MbZ6AEmHfS83ADHbH1e-u4fjrHqRymc_AM61ZhD-C',\n    'service_id': 'iam-ServiceId-de7629bb-45be-4b02-9da7-963fa26c02ba',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n\nconfiguration_name = 'os_c4f7ec2404e542d7975c82953f07934a_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'false')\\\n  .load(cos.url('ratings_Books.csv', 'project13bigdataca7d76b8c08445c79249a515ffeb56cb'))"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "[Row(User='AH2L9G3DQHHAJ', ProductId='0000000116', Rating='4.0', epoch='1019865600'),\n Row(User='A2IIIDRK3PRRZY', ProductId='0000000116', Rating='1.0', epoch='1395619200'),\n Row(User='A1TADCM7YWPQ8M', ProductId='0000000868', Rating='4.0', epoch='1031702400'),\n Row(User='AWGH7V0BDOJKB', ProductId='0000013714', Rating='4.0', epoch='1383177600'),\n Row(User='A3UTQPQPM4TQO0', ProductId='0000013714', Rating='5.0', epoch='1374883200')]"
                    }
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 5, 
            "source": "df = df.withColumnRenamed(\"_c0\",\"User\")\ndf = df.withColumnRenamed(\"_c1\",\"ProductId\")\ndf = df.withColumnRenamed(\"_c2\",\"Rating\")\ndf = df.withColumnRenamed(\"_c3\",\"epoch\")\ndf.take(5)"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- User: string (nullable = true)\n |-- ProductId: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- epoch: string (nullable = true)\n |-- timestamp: string (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 40, 
            "source": "df.printSchema()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Timestamp Conversion from epoch to string"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 7, 
            "source": "from pyspark.sql.functions import *\ndf = df.withColumn(\"timestamp\",from_unixtime(col(\"epoch\")))"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- User: string (nullable = true)\n |-- ProductId: string (nullable = true)\n |-- Rating: string (nullable = true)\n |-- epoch: string (nullable = true)\n |-- timestamp: string (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 38, 
            "source": "df.printSchema()"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 9, 
                    "data": {
                        "text/plain": "[Row(User='AH2L9G3DQHHAJ', ProductId='0000000116', Rating='4.0', epoch='1019865600', timestamp='2002-04-26 19:00:00'),\n Row(User='A2IIIDRK3PRRZY', ProductId='0000000116', Rating='1.0', epoch='1395619200', timestamp='2014-03-23 19:00:00'),\n Row(User='A1TADCM7YWPQ8M', ProductId='0000000868', Rating='4.0', epoch='1031702400', timestamp='2002-09-10 19:00:00'),\n Row(User='AWGH7V0BDOJKB', ProductId='0000013714', Rating='4.0', epoch='1383177600', timestamp='2013-10-30 19:00:00'),\n Row(User='A3UTQPQPM4TQO0', ProductId='0000013714', Rating='5.0', epoch='1374883200', timestamp='2013-07-26 19:00:00')]"
                    }
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 9, 
            "source": "df.take(5)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Data Analysis"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "We grouped the data on Rating and calculated the count to analyze how many were of each rating. From the table below, we discovered that users gave an overwhelming majority of 5.0 ratings over any other rating. Additionally ratings leaned more toward higher ratings; however the number of people who gave the worst rating is more than the number who gave a 2.0 rating."
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 22, 
            "source": "gr = df.groupBy(\"Rating\").count().sort(\"Rating\")"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 2, 
            "source": "# Build the SQL context required to create a Spark dataframe \nsqlContext=SQLContext(sc) "
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Products with most ratings"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "In the below query, we analyze the ten products that were given the largest number of ratings, sorted in descending order."
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+----------+-----+\n| ProductId|count|\n+----------+-----+\n|0439023483|21398|\n|030758836X|19867|\n|0439023513|14114|\n|0385537859|12973|\n|0007444117|12629|\n|0375831002|12571|\n|038536315X|12564|\n|0345803485|12290|\n|0316055433|11746|\n|0849922070|10424|\n+----------+-----+\nonly showing top 10 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 41, 
            "source": "df.groupBy(\"ProductId\").count().sort(col(\"count\").desc()).show(10)"
        }, 
        {
            "outputs": [], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 42, 
            "source": "df_2 = df.withColumn(\"year\", col(\"timestamp\").substr(0,4))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Ratings per year"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+----+-------+\n|year|  count|\n+----+-------+\n|2014|4728411|\n|2013|6994009|\n|2012|2931573|\n|2011|1525596|\n|2010|1131037|\n|2009| 977937|\n|2008| 783409|\n|2007| 713494|\n|2006| 543011|\n|2005| 479083|\n|2004| 337309|\n|2003| 288616|\n|2002| 284875|\n|2001| 295022|\n|2000| 322493|\n|1999| 103225|\n|1998|  55800|\n|1997|  12218|\n|1996|     37|\n+----+-------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 45, 
            "source": "df_2.groupBy(\"year\").count().sort(col(\"year\").desc()).show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Rating counts per year ordered by count"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+----+-------+\n|year|  count|\n+----+-------+\n|2013|6994009|\n|2014|4728411|\n|2012|2931573|\n|2011|1525596|\n|2010|1131037|\n|2009| 977937|\n|2008| 783409|\n|2007| 713494|\n|2006| 543011|\n|2005| 479083|\n|2004| 337309|\n|2000| 322493|\n|2001| 295022|\n|2003| 288616|\n|2002| 284875|\n|1999| 103225|\n|1998|  55800|\n|1997|  12218|\n|1996|     37|\n+----+-------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 47, 
            "source": "df_2.groupBy(\"year\").count().sort(col(\"count\").desc()).show()"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+----+--------------+-----+\n|year|          User|count|\n+----+--------------+-----+\n|2006| AFYCJ0BY9XX84|    1|\n|2014|A1BWIXPFCVF2RM|    1|\n|2007| ADE36E3UI2T08|    6|\n|2007|A2B63ED0F870AT|    1|\n|2013| AN5Q5TGABA5WY|    3|\n|2008|A1EIFIMA43D8UN|    1|\n|2014|A2CD9BZSYUJZ1Q|    4|\n|2014|A3TRELWZQHPSVT|    3|\n|2013|A2LDDHKECKIM1Y|   12|\n|2008|A2MVUWT453QH61|   49|\n|2010|A3CQ5X4DM1GCK7|    8|\n|2010|A25FW4MUV5206K|    2|\n|2010|A269FUWRKV5M7N|    2|\n|2009|A27KA2H3E5C14S|    1|\n|2014|A1QVWXDUMVDMJ2|    2|\n|2011|A2NX1VF0HK4BSR|    6|\n|2013| A8I6ZL027SCFZ|    2|\n|2013|A3LI3BS69XXNBG|    1|\n|2000|  ANKOIBS1MV5H|    3|\n|2000| ACELKH5KHQ752|    1|\n+----+--------------+-----+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 51, 
            "source": "counts = df_2.groupBy(['year', 'User']).count().alias('counts')\ncounts.show(20)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Rating providers per year"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+----+-------+\n|year|  count|\n+----+-------+\n|2014|2224276|\n|2013|3000014|\n|2012|1495209|\n|2011| 815658|\n|2010| 625652|\n|2009| 539602|\n|2008| 430136|\n|2007| 394424|\n|2006| 298852|\n|2005| 262755|\n|2004| 179255|\n|2003| 152942|\n|2002| 148924|\n|2001| 157058|\n|2000| 184189|\n|1999|  74670|\n|1998|  39499|\n|1997|   9053|\n|1996|     13|\n+----+-------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "cell_type": "code", 
            "execution_count": 83, 
            "source": "counts.groupBy('year').count().sort(col(\"year\").desc()).show()"
        }
    ], 
    "nbformat_minor": 1
}